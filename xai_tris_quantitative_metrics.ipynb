{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0892294-9eb8-419b-a6a4-8de88d8ccaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Aggregated results (means ± stds) ===\n",
      "[linear_additive_1d1p_0.10_correlated]\n",
      "  DISCR_NAM: IMA=0.797±0.067, EMD=0.905±0.026, FNI-EMD=0.932±0.023\n",
      "  DISCR_QLR: IMA=0.665±0.054, EMD=0.872±0.021, FNI-EMD=0.886±0.019\n",
      "  PROD_NAM: IMA=0.955±0.026, EMD=0.835±0.072, FNI-EMD=0.991±0.006\n",
      "  PROD_QLR: IMA=0.961±0.022, EMD=0.975±0.015, FNI-EMD=0.990±0.005\n",
      "  SD_NAM: IMA=0.483±0.051, EMD=0.788±0.066, FNI-EMD=0.889±0.022\n",
      "  SD_QLR: IMA=0.487±0.002, EMD=0.862±0.001, FNI-EMD=0.869±0.001\n",
      "  SDb_NAM: IMA=0.850±0.065, EMD=0.926±0.027, FNI-EMD=0.956±0.020\n",
      "  SDb_QLR: IMA=0.804±0.098, EMD=0.924±0.036, FNI-EMD=0.936±0.032\n",
      "  ebm: IMA=0.566±0.003, EMD=0.838±0.011, FNI-EMD=0.904±0.001\n",
      "  ig: IMA=0.857±0.036, EMD=0.936±0.024, FNI-EMD=0.960±0.009\n",
      "  kernel_svm: IMA=0.797±0.075, EMD=0.917±0.027, FNI-EMD=0.927±0.023\n",
      "  pattern_attribution: IMA=0.418±0.171, EMD=0.749±0.063, FNI-EMD=0.805±0.071\n",
      "  pattern_gam: IMA=0.395±0.095, EMD=0.775±0.035, FNI-EMD=0.800±0.035\n",
      "  pattern_net: IMA=0.397±0.191, EMD=0.776±0.068, FNI-EMD=0.792±0.068\n",
      "  pattern_qlr: IMA=0.520±0.066, EMD=0.820±0.030, FNI-EMD=0.838±0.027\n",
      "  shap: IMA=0.448±0.041, EMD=0.756±0.072, FNI-EMD=0.851±0.024\n",
      "\n",
      "[linear_additive_1d1p_0.20_white]\n",
      "  DISCR_NAM: IMA=0.899±0.014, EMD=0.946±0.023, FNI-EMD=0.966±0.005\n",
      "  DISCR_QLR: IMA=0.743±0.005, EMD=0.906±0.003, FNI-EMD=0.913±0.002\n",
      "  PROD_NAM: IMA=0.998±0.001, EMD=0.962±0.030, FNI-EMD=0.999±0.000\n",
      "  PROD_QLR: IMA=0.982±0.001, EMD=0.977±0.006, FNI-EMD=0.994±0.000\n",
      "  SD_NAM: IMA=0.935±0.038, EMD=0.960±0.023, FNI-EMD=0.978±0.013\n",
      "  SD_QLR: IMA=0.594±0.011, EMD=0.851±0.005, FNI-EMD=0.862±0.005\n",
      "  SDb_NAM: IMA=0.972±0.019, EMD=0.970±0.023, FNI-EMD=0.990±0.007\n",
      "  SDb_QLR: IMA=0.785±0.004, EMD=0.921±0.004, FNI-EMD=0.927±0.003\n",
      "  ebm: IMA=0.884±0.012, EMD=0.957±0.005, FNI-EMD=0.961±0.005\n",
      "  ig: IMA=0.971±0.009, EMD=0.964±0.038, FNI-EMD=0.990±0.004\n",
      "  kernel_svm: IMA=0.893±0.007, EMD=0.960±0.003, FNI-EMD=0.964±0.002\n",
      "  pattern_attribution: IMA=0.961±0.048, EMD=0.939±0.052, FNI-EMD=0.987±0.016\n",
      "  pattern_gam: IMA=0.864±0.018, EMD=0.943±0.015, FNI-EMD=0.954±0.007\n",
      "  pattern_net: IMA=0.711±0.156, EMD=0.888±0.060, FNI-EMD=0.902±0.054\n",
      "  pattern_qlr: IMA=0.703±0.007, EMD=0.891±0.002, FNI-EMD=0.899±0.002\n",
      "  shap: IMA=0.736±0.056, EMD=0.839±0.041, FNI-EMD=0.909±0.020\n",
      "\n",
      "[linear_distractor_additive_1d1p_0.05_0.47_0.47_correlated]\n",
      "  DISCR_NAM: IMA=0.708±0.070, EMD=0.861±0.026, FNI-EMD=0.903±0.025\n",
      "  DISCR_QLR: IMA=0.557±0.047, EMD=0.834±0.018, FNI-EMD=0.852±0.019\n",
      "  PROD_NAM: IMA=0.941±0.025, EMD=0.735±0.119, FNI-EMD=0.988±0.005\n",
      "  PROD_QLR: IMA=0.929±0.021, EMD=0.853±0.017, FNI-EMD=0.985±0.005\n",
      "  SD_NAM: IMA=0.461±0.041, EMD=0.712±0.108, FNI-EMD=0.881±0.018\n",
      "  SD_QLR: IMA=0.486±0.002, EMD=0.832±0.001, FNI-EMD=0.889±0.001\n",
      "  SDb_NAM: IMA=0.779±0.066, EMD=0.888±0.030, FNI-EMD=0.936±0.021\n",
      "  SDb_QLR: IMA=0.662±0.082, EMD=0.859±0.023, FNI-EMD=0.889±0.029\n",
      "  ebm: IMA=0.524±0.002, EMD=0.791±0.007, FNI-EMD=0.906±0.001\n",
      "  ig: IMA=0.797±0.050, EMD=0.811±0.039, FNI-EMD=0.948±0.013\n",
      "  kernel_svm: IMA=0.738±0.056, EMD=0.893±0.019, FNI-EMD=0.905±0.018\n",
      "  pattern_attribution: IMA=0.373±0.131, EMD=0.747±0.065, FNI-EMD=0.801±0.066\n",
      "  pattern_gam: IMA=0.364±0.113, EMD=0.770±0.046, FNI-EMD=0.797±0.042\n",
      "  pattern_net: IMA=0.316±0.141, EMD=0.742±0.052, FNI-EMD=0.761±0.054\n",
      "  pattern_qlr: IMA=0.366±0.035, EMD=0.771±0.021, FNI-EMD=0.790±0.022\n",
      "  shap: IMA=0.431±0.034, EMD=0.765±0.095, FNI-EMD=0.853±0.023\n",
      "\n",
      "[linear_distractor_additive_1d1p_0.20_0.40_0.40_white]\n",
      "  DISCR_NAM: IMA=0.910±0.014, EMD=0.892±0.024, FNI-EMD=0.970±0.005\n",
      "  DISCR_QLR: IMA=0.774±0.005, EMD=0.895±0.005, FNI-EMD=0.924±0.003\n",
      "  PROD_NAM: IMA=0.996±0.002, EMD=0.824±0.047, FNI-EMD=0.999±0.000\n",
      "  PROD_QLR: IMA=0.991±0.002, EMD=0.821±0.012, FNI-EMD=0.998±0.000\n",
      "  SD_NAM: IMA=0.764±0.037, EMD=0.893±0.041, FNI-EMD=0.957±0.007\n",
      "  SD_QLR: IMA=0.547±0.016, EMD=0.858±0.008, FNI-EMD=0.881±0.008\n",
      "  SDb_NAM: IMA=0.987±0.006, EMD=0.889±0.032, FNI-EMD=0.997±0.002\n",
      "  SDb_QLR: IMA=0.859±0.003, EMD=0.833±0.005, FNI-EMD=0.952±0.002\n",
      "  ebm: IMA=0.632±0.011, EMD=0.903±0.005, FNI-EMD=0.919±0.003\n",
      "  ig: IMA=0.949±0.014, EMD=0.842±0.046, FNI-EMD=0.987±0.003\n",
      "  kernel_svm: IMA=0.930±0.008, EMD=0.974±0.004, FNI-EMD=0.976±0.003\n",
      "  pattern_attribution: IMA=0.933±0.076, EMD=0.861±0.067, FNI-EMD=0.977±0.028\n",
      "  pattern_gam: IMA=0.890±0.013, EMD=0.924±0.018, FNI-EMD=0.963±0.005\n",
      "  pattern_net: IMA=0.802±0.112, EMD=0.897±0.059, FNI-EMD=0.940±0.033\n",
      "  pattern_qlr: IMA=0.744±0.005, EMD=0.887±0.003, FNI-EMD=0.913±0.003\n",
      "  shap: IMA=0.538±0.090, EMD=0.819±0.042, FNI-EMD=0.882±0.026\n",
      "\n",
      "[multiplicative_distractor_multiplicative_1d1p_0.40_0.30_0.30_correlated]\n",
      "  DISCR_NAM: IMA=0.636±0.095, EMD=0.852±0.034, FNI-EMD=0.880±0.034\n",
      "  DISCR_QLR: IMA=0.299±0.105, EMD=0.749±0.027, FNI-EMD=0.769±0.035\n",
      "  PROD_NAM: IMA=0.952±0.027, EMD=0.816±0.075, FNI-EMD=0.990±0.005\n",
      "  PROD_QLR: IMA=0.475±0.080, EMD=0.816±0.027, FNI-EMD=0.831±0.033\n",
      "  SD_NAM: IMA=0.476±0.045, EMD=0.783±0.063, FNI-EMD=0.893±0.014\n",
      "  SD_QLR: IMA=0.241±0.027, EMD=0.743±0.017, FNI-EMD=0.753±0.017\n",
      "  SDb_NAM: IMA=0.846±0.066, EMD=0.910±0.032, FNI-EMD=0.958±0.018\n",
      "  SDb_QLR: IMA=0.286±0.029, EMD=0.753±0.030, FNI-EMD=0.768±0.027\n",
      "  ebm: IMA=0.461±0.005, EMD=0.872±0.004, FNI-EMD=0.894±0.003\n",
      "  ig: IMA=0.477±0.007, EMD=0.818±0.024, FNI-EMD=0.876±0.003\n",
      "  kernel_svm: IMA=0.087±0.028, EMD=0.607±0.046, FNI-EMD=0.625±0.051\n",
      "  pattern_attribution: IMA=0.394±0.119, EMD=0.770±0.060, FNI-EMD=0.829±0.055\n",
      "  pattern_gam: IMA=0.385±0.114, EMD=0.758±0.054, FNI-EMD=0.809±0.036\n",
      "  pattern_net: IMA=0.120±0.039, EMD=0.682±0.018, FNI-EMD=0.695±0.018\n",
      "  pattern_qlr: IMA=0.216±0.041, EMD=0.730±0.027, FNI-EMD=0.746±0.025\n",
      "  shap: IMA=0.389±0.075, EMD=0.762±0.046, FNI-EMD=0.823±0.035\n",
      "\n",
      "[multiplicative_distractor_multiplicative_1d1p_0.60_0.20_0.20_white]\n",
      "  DISCR_NAM: IMA=0.748±0.031, EMD=0.889±0.022, FNI-EMD=0.915±0.012\n",
      "  DISCR_QLR: IMA=0.120±0.012, EMD=0.677±0.009, FNI-EMD=0.687±0.007\n",
      "  PROD_NAM: IMA=0.994±0.012, EMD=0.914±0.054, FNI-EMD=0.998±0.003\n",
      "  PROD_QLR: IMA=0.108±0.016, EMD=0.669±0.015, FNI-EMD=0.678±0.014\n",
      "  SD_NAM: IMA=0.903±0.043, EMD=0.927±0.037, FNI-EMD=0.968±0.015\n",
      "  SD_QLR: IMA=0.122±0.021, EMD=0.678±0.012, FNI-EMD=0.688±0.014\n",
      "  SDb_NAM: IMA=0.942±0.032, EMD=0.935±0.038, FNI-EMD=0.981±0.011\n",
      "  SDb_QLR: IMA=0.123±0.011, EMD=0.678±0.010, FNI-EMD=0.688±0.010\n",
      "  ebm: IMA=0.873±0.009, EMD=0.948±0.003, FNI-EMD=0.958±0.003\n",
      "  ig: IMA=0.930±0.018, EMD=0.920±0.048, FNI-EMD=0.976±0.006\n",
      "  kernel_svm: IMA=0.092±0.016, EMD=0.668±0.015, FNI-EMD=0.677±0.015\n",
      "  pattern_attribution: IMA=0.863±0.099, EMD=0.760±0.095, FNI-EMD=0.954±0.032\n",
      "  pattern_gam: IMA=0.817±0.034, EMD=0.913±0.026, FNI-EMD=0.938±0.012\n",
      "  pattern_net: IMA=0.341±0.089, EMD=0.743±0.032, FNI-EMD=0.771±0.036\n",
      "  pattern_qlr: IMA=0.164±0.018, EMD=0.691±0.008, FNI-EMD=0.703±0.008\n",
      "  shap: IMA=0.644±0.073, EMD=0.828±0.034, FNI-EMD=0.879±0.028\n",
      "\n",
      "[multiplicative_multiplicative_1d1p_0.40_correlated]\n",
      "  DISCR_NAM: IMA=0.643±0.102, EMD=0.854±0.036, FNI-EMD=0.882±0.035\n",
      "  DISCR_QLR: IMA=0.301±0.111, EMD=0.750±0.035, FNI-EMD=0.769±0.040\n",
      "  PROD_NAM: IMA=0.952±0.030, EMD=0.854±0.050, FNI-EMD=0.990±0.006\n",
      "  PROD_QLR: IMA=0.482±0.060, EMD=0.819±0.018, FNI-EMD=0.836±0.024\n",
      "  SD_NAM: IMA=0.461±0.051, EMD=0.809±0.042, FNI-EMD=0.887±0.016\n",
      "  SD_QLR: IMA=0.253±0.035, EMD=0.749±0.019, FNI-EMD=0.760±0.020\n",
      "  SDb_NAM: IMA=0.843±0.072, EMD=0.912±0.031, FNI-EMD=0.956±0.020\n",
      "  SDb_QLR: IMA=0.328±0.107, EMD=0.761±0.042, FNI-EMD=0.778±0.045\n",
      "  ebm: IMA=0.466±0.005, EMD=0.884±0.004, FNI-EMD=0.896±0.003\n",
      "  ig: IMA=0.481±0.005, EMD=0.860±0.009, FNI-EMD=0.872±0.003\n",
      "  kernel_svm: IMA=0.094±0.030, EMD=0.615±0.045, FNI-EMD=0.633±0.051\n",
      "  pattern_attribution: IMA=0.369±0.120, EMD=0.781±0.054, FNI-EMD=0.817±0.048\n",
      "  pattern_gam: IMA=0.360±0.112, EMD=0.758±0.044, FNI-EMD=0.801±0.038\n",
      "  pattern_net: IMA=0.111±0.027, EMD=0.682±0.012, FNI-EMD=0.691±0.012\n",
      "  pattern_qlr: IMA=0.214±0.023, EMD=0.728±0.021, FNI-EMD=0.743±0.018\n",
      "  shap: IMA=0.389±0.095, EMD=0.777±0.046, FNI-EMD=0.821±0.040\n",
      "\n",
      "[multiplicative_multiplicative_1d1p_0.60_white]\n",
      "  DISCR_NAM: IMA=0.744±0.032, EMD=0.890±0.020, FNI-EMD=0.913±0.012\n",
      "  DISCR_QLR: IMA=0.119±0.014, EMD=0.676±0.009, FNI-EMD=0.686±0.008\n",
      "  PROD_NAM: IMA=0.994±0.014, EMD=0.912±0.061, FNI-EMD=0.998±0.005\n",
      "  PROD_QLR: IMA=0.105±0.018, EMD=0.669±0.013, FNI-EMD=0.678±0.012\n",
      "  SD_NAM: IMA=0.904±0.034, EMD=0.928±0.033, FNI-EMD=0.967±0.012\n",
      "  SD_QLR: IMA=0.116±0.024, EMD=0.680±0.013, FNI-EMD=0.689±0.013\n",
      "  SDb_NAM: IMA=0.940±0.026, EMD=0.936±0.035, FNI-EMD=0.980±0.009\n",
      "  SDb_QLR: IMA=0.120±0.013, EMD=0.677±0.011, FNI-EMD=0.686±0.011\n",
      "  ebm: IMA=0.885±0.011, EMD=0.956±0.004, FNI-EMD=0.960±0.003\n",
      "  ig: IMA=0.932±0.017, EMD=0.919±0.052, FNI-EMD=0.977±0.006\n",
      "  kernel_svm: IMA=0.092±0.019, EMD=0.667±0.016, FNI-EMD=0.676±0.016\n",
      "  pattern_attribution: IMA=0.846±0.117, EMD=0.769±0.098, FNI-EMD=0.947±0.047\n",
      "  pattern_gam: IMA=0.810±0.042, EMD=0.912±0.035, FNI-EMD=0.936±0.015\n",
      "  pattern_net: IMA=0.324±0.091, EMD=0.735±0.032, FNI-EMD=0.763±0.037\n",
      "  pattern_qlr: IMA=0.160±0.018, EMD=0.690±0.007, FNI-EMD=0.701±0.007\n",
      "  shap: IMA=0.647±0.068, EMD=0.821±0.043, FNI-EMD=0.879±0.026\n",
      "\n",
      "[xor_additive_1d1p_0.30_correlated]\n",
      "  DISCR_NAM: IMA=0.120±0.044, EMD=0.659±0.038, FNI-EMD=0.671±0.039\n",
      "  DISCR_QLR: IMA=0.770±0.035, EMD=0.719±0.114, FNI-EMD=0.922±0.014\n",
      "  PROD_NAM: IMA=0.444±0.185, EMD=0.704±0.093, FNI-EMD=0.805±0.089\n",
      "  PROD_QLR: IMA=0.996±0.002, EMD=0.653±0.142, FNI-EMD=0.999±0.001\n",
      "  SD_NAM: IMA=0.530±0.164, EMD=0.746±0.070, FNI-EMD=0.849±0.067\n",
      "  SD_QLR: IMA=0.682±0.080, EMD=0.747±0.090, FNI-EMD=0.906±0.026\n",
      "  SDb_NAM: IMA=0.200±0.086, EMD=0.680±0.060, FNI-EMD=0.710±0.060\n",
      "  SDb_QLR: IMA=0.930±0.036, EMD=0.683±0.130, FNI-EMD=0.976±0.012\n",
      "  ebm: IMA=0.130±0.045, EMD=0.676±0.032, FNI-EMD=0.687±0.034\n",
      "  ig: IMA=0.759±0.122, EMD=0.876±0.063, FNI-EMD=0.929±0.035\n",
      "  kernel_svm: IMA=0.107±0.026, EMD=0.616±0.044, FNI-EMD=0.639±0.053\n",
      "  pattern_attribution: IMA=0.780±0.192, EMD=0.818±0.088, FNI-EMD=0.927±0.068\n",
      "  pattern_gam: IMA=0.259±0.088, EMD=0.727±0.041, FNI-EMD=0.750±0.041\n",
      "  pattern_net: IMA=0.554±0.203, EMD=0.792±0.078, FNI-EMD=0.848±0.074\n",
      "  pattern_qlr: IMA=0.591±0.050, EMD=0.783±0.062, FNI-EMD=0.872±0.024\n",
      "  shap: IMA=0.365±0.088, EMD=0.746±0.046, FNI-EMD=0.794±0.046\n",
      "\n",
      "[xor_additive_1d1p_0.30_white]\n",
      "  DISCR_NAM: IMA=0.097±0.030, EMD=0.662±0.025, FNI-EMD=0.671±0.026\n",
      "  DISCR_QLR: IMA=0.765±0.019, EMD=0.861±0.040, FNI-EMD=0.921±0.006\n",
      "  PROD_NAM: IMA=0.588±0.172, EMD=0.761±0.078, FNI-EMD=0.853±0.078\n",
      "  PROD_QLR: IMA=0.994±0.001, EMD=0.876±0.041, FNI-EMD=0.998±0.000\n",
      "  SD_NAM: IMA=0.691±0.120, EMD=0.827±0.045, FNI-EMD=0.897±0.042\n",
      "  SD_QLR: IMA=0.611±0.037, EMD=0.843±0.021, FNI-EMD=0.868±0.011\n",
      "  SDb_NAM: IMA=0.227±0.092, EMD=0.683±0.058, FNI-EMD=0.712±0.063\n",
      "  SDb_QLR: IMA=0.879±0.011, EMD=0.861±0.052, FNI-EMD=0.959±0.004\n",
      "  ebm: IMA=0.091±0.018, EMD=0.661±0.011, FNI-EMD=0.669±0.011\n",
      "  ig: IMA=0.822±0.082, EMD=0.865±0.053, FNI-EMD=0.940±0.029\n",
      "  kernel_svm: IMA=0.122±0.031, EMD=0.678±0.017, FNI-EMD=0.689±0.018\n",
      "  pattern_attribution: IMA=0.977±0.021, EMD=0.856±0.082, FNI-EMD=0.992±0.007\n",
      "  pattern_gam: IMA=0.187±0.054, EMD=0.696±0.027, FNI-EMD=0.711±0.029\n",
      "  pattern_net: IMA=0.753±0.152, EMD=0.851±0.084, FNI-EMD=0.915±0.054\n",
      "  pattern_qlr: IMA=0.743±0.021, EMD=0.869±0.036, FNI-EMD=0.913±0.006\n",
      "  shap: IMA=0.605±0.085, EMD=0.827±0.030, FNI-EMD=0.866±0.035\n",
      "\n",
      "[xor_distractor_additive_1d1p_0.20_0.40_0.40_correlated]\n",
      "  DISCR_NAM: IMA=0.128±0.055, EMD=0.661±0.032, FNI-EMD=0.676±0.037\n",
      "  DISCR_QLR: IMA=0.720±0.056, EMD=0.768±0.084, FNI-EMD=0.905±0.019\n",
      "  PROD_NAM: IMA=0.482±0.228, EMD=0.701±0.097, FNI-EMD=0.829±0.101\n",
      "  PROD_QLR: IMA=0.993±0.003, EMD=0.716±0.098, FNI-EMD=0.998±0.001\n",
      "  SD_NAM: IMA=0.533±0.190, EMD=0.748±0.074, FNI-EMD=0.856±0.073\n",
      "  SD_QLR: IMA=0.645±0.046, EMD=0.824±0.046, FNI-EMD=0.914±0.011\n",
      "  SDb_NAM: IMA=0.255±0.110, EMD=0.692±0.068, FNI-EMD=0.745±0.071\n",
      "  SDb_QLR: IMA=0.886±0.047, EMD=0.767±0.099, FNI-EMD=0.961±0.015\n",
      "  ebm: IMA=0.137±0.033, EMD=0.672±0.020, FNI-EMD=0.685±0.021\n",
      "  ig: IMA=0.767±0.119, EMD=0.862±0.051, FNI-EMD=0.937±0.032\n",
      "  kernel_svm: IMA=0.137±0.051, EMD=0.626±0.046, FNI-EMD=0.650±0.056\n",
      "  pattern_attribution: IMA=0.818±0.115, EMD=0.815±0.082, FNI-EMD=0.952±0.035\n",
      "  pattern_gam: IMA=0.240±0.104, EMD=0.720±0.049, FNI-EMD=0.742±0.054\n",
      "  pattern_net: IMA=0.560±0.209, EMD=0.798±0.077, FNI-EMD=0.854±0.073\n",
      "  pattern_qlr: IMA=0.599±0.088, EMD=0.786±0.063, FNI-EMD=0.876±0.035\n",
      "  shap: IMA=0.381±0.048, EMD=0.756±0.039, FNI-EMD=0.821±0.031\n",
      "\n",
      "[xor_distractor_additive_1d1p_0.20_0.40_0.40_white]\n",
      "  DISCR_NAM: IMA=0.108±0.034, EMD=0.663±0.028, FNI-EMD=0.674±0.029\n",
      "  DISCR_QLR: IMA=0.680±0.041, EMD=0.804±0.049, FNI-EMD=0.892±0.015\n",
      "  PROD_NAM: IMA=0.547±0.186, EMD=0.735±0.077, FNI-EMD=0.832±0.092\n",
      "  PROD_QLR: IMA=0.987±0.004, EMD=0.712±0.132, FNI-EMD=0.996±0.001\n",
      "  SD_NAM: IMA=0.653±0.132, EMD=0.797±0.055, FNI-EMD=0.882±0.053\n",
      "  SD_QLR: IMA=0.508±0.049, EMD=0.807±0.023, FNI-EMD=0.851±0.014\n",
      "  SDb_NAM: IMA=0.244±0.108, EMD=0.675±0.066, FNI-EMD=0.711±0.071\n",
      "  SDb_QLR: IMA=0.813±0.032, EMD=0.781±0.092, FNI-EMD=0.936±0.011\n",
      "  ebm: IMA=0.090±0.017, EMD=0.662±0.011, FNI-EMD=0.670±0.011\n",
      "  ig: IMA=0.672±0.114, EMD=0.839±0.048, FNI-EMD=0.911±0.030\n",
      "  kernel_svm: IMA=0.156±0.035, EMD=0.677±0.026, FNI-EMD=0.692±0.031\n",
      "  pattern_attribution: IMA=0.927±0.061, EMD=0.870±0.068, FNI-EMD=0.978±0.022\n",
      "  pattern_gam: IMA=0.304±0.093, EMD=0.739±0.038, FNI-EMD=0.761±0.041\n",
      "  pattern_net: IMA=0.739±0.139, EMD=0.847±0.068, FNI-EMD=0.911±0.050\n",
      "  pattern_qlr: IMA=0.670±0.046, EMD=0.792±0.072, FNI-EMD=0.888±0.017\n",
      "  shap: IMA=0.492±0.070, EMD=0.804±0.041, FNI-EMD=0.859±0.022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, pickle as pkl, warnings\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from ot.lp import emd\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# =========================\n",
    "# Switches\n",
    "# =========================\n",
    "# If True: use IMP-SASI (credit pair only to the stronger-univariate feature)\n",
    "# If False: fall back to MAX rule (credit max(|pair|, |uni|) to BOTH ends)\n",
    "USE_SASI_FOR_PRIMARY = True   # affects pattern_gam / pattern_qlr / ebm (when (192,) or (2144,) shape)\n",
    "USE_SASI_FOR_TILDE   = True   # affects SD/SDb/DISCR/PROD for NAM & QLR tilde metrics\n",
    "\n",
    "# =========================\n",
    "# FAST (interaction pairs)\n",
    "# =========================\n",
    "from interpret.utils import measure_interactions\n",
    "def FAST(X_train, y_train, n_interactions, init_score=None, feature_names=None, feature_types=None):\n",
    "    interactions = measure_interactions(\n",
    "        X_train, y_train, interactions=n_interactions,\n",
    "        init_score=init_score, feature_names=feature_names, feature_types=feature_types\n",
    "    )\n",
    "    pairs = []\n",
    "    for (i, j), _ in interactions:\n",
    "        pairs.append((i, j))\n",
    "    return pairs, None\n",
    "\n",
    "# =========================\n",
    "# Metrics\n",
    "# =========================\n",
    "def importance_mass_accuracy(gt_mask, attribution):\n",
    "    if not isinstance(gt_mask, np.ndarray) or not isinstance(attribution, np.ndarray):\n",
    "        return np.nan\n",
    "    if attribution.ndim != 1 or len(gt_mask) != len(attribution):\n",
    "        return np.nan\n",
    "    abs_attr = np.abs(attribution)\n",
    "    total = float(np.sum(abs_attr))\n",
    "    if total == 0.0:\n",
    "        return 1.0 if np.sum(abs_attr[gt_mask == 1]) == 0 else 0.0\n",
    "    return float(np.sum(abs_attr[gt_mask == 1]) / total)\n",
    "\n",
    "def create_cost_matrix(edge):\n",
    "    if edge <= 0: return np.zeros((0,0))\n",
    "    if edge == 1: return np.array([[0.0]])\n",
    "    rr, cc = np.indices((edge, edge))\n",
    "    coords = np.column_stack([rr.ravel(), cc.ravel()])\n",
    "    return cdist(coords, coords)\n",
    "\n",
    "def _tight_ot_denominator(gt_dist, cost_matrix):\n",
    "    exp_dists = cost_matrix @ gt_dist\n",
    "    return float(np.max(exp_dists))\n",
    "\n",
    "def calculate_emd_score_metric(gt_mask_flat, attribution_flat, grid_edge_length, base_cost_matrix, is_fni=False):\n",
    "    if not (isinstance(gt_mask_flat, np.ndarray) and isinstance(attribution_flat, np.ndarray)): return np.nan\n",
    "    if gt_mask_flat.ndim != 1 or attribution_flat.ndim != 1: return np.nan\n",
    "    if len(gt_mask_flat) != len(attribution_flat) or len(gt_mask_flat) != grid_edge_length * grid_edge_length: return np.nan\n",
    "\n",
    "    C = np.array(base_cost_matrix, copy=True)\n",
    "    if is_fni:\n",
    "        idx = np.where(gt_mask_flat == 1)[0]\n",
    "        if idx.size:\n",
    "            C[np.ix_(idx, idx)] = 0.0\n",
    "\n",
    "    t = gt_mask_flat.astype(np.float64)\n",
    "    s = np.abs(attribution_flat).astype(np.float64)\n",
    "    sum_t, sum_s = float(t.sum()), float(s.sum())\n",
    "\n",
    "    if sum_t < 1e-9 and sum_s < 1e-9: return 1.0\n",
    "    if sum_t < 1e-9 or sum_s < 1e-9:  return 0.0\n",
    "\n",
    "    q = t / sum_t\n",
    "    p = s / sum_s\n",
    "\n",
    "    try:\n",
    "        _, log = emd(np.ascontiguousarray(p), np.ascontiguousarray(q),\n",
    "                     np.ascontiguousarray(C), numItermax=200000, log=True)\n",
    "        ot_cost = float(log[\"cost\"])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "    denom = _tight_ot_denominator(q, C)\n",
    "    if denom <= 1e-12:\n",
    "        d_max = np.sqrt(2 * (grid_edge_length - 1)**2) if grid_edge_length > 1 else 1.0\n",
    "        denom = max(d_max, 1.0)\n",
    "\n",
    "    score = 1.0 - (ot_cost / denom)\n",
    "    return float(np.clip(score, 0.0, 1.0))\n",
    "\n",
    "# =========================\n",
    "# Ground truth (8x8)\n",
    "# =========================\n",
    "D_2D_EDGE = 8\n",
    "D = D_2D_EDGE * D_2D_EDGE\n",
    "normal_t = np.array([[1,0],[1,1],[1,0]])\n",
    "normal_l = np.array([[1,0],[1,0],[1,1]])\n",
    "GT_MASK_2D = np.zeros((D_2D_EDGE, D_2D_EDGE), dtype=int)\n",
    "GT_MASK_2D[1:4, 1:3] = normal_t\n",
    "GT_MASK_2D[4:7, 5:7] = normal_l\n",
    "GT_MASK_2D_FLAT = GT_MASK_2D.flatten()\n",
    "COST_MATRIX_MAIN_EFFECTS = create_cost_matrix(D_2D_EDGE)\n",
    "\n",
    "# =========================\n",
    "# IO\n",
    "# =========================\n",
    "\n",
    "# tilde is the old naming for \\tilde{f}_i <- w_i f_i; the final version used z_i <- w_i f_i but I haven't (yet) changed it here\n",
    "# There are two separate files are because the original explanations did not need to be changed\n",
    "PRIMARY_PATH = './models/xai_tris/explanations_cameraready.pkl'\n",
    "TILDE_PATH   = './models/xai_tris/explanations_cameraready_nam_tilde.pkl'\n",
    "with open(PRIMARY_PATH, 'rb') as f:\n",
    "    explanations = pkl.load(f)\n",
    "with open(TILDE_PATH, 'rb') as f:\n",
    "    explanations_nam = pkl.load(f)\n",
    "\n",
    "# =========================\n",
    "# Scenario helpers\n",
    "# =========================\n",
    "def base_scenario_name(key: str):\n",
    "    m = re.match(r\"^(.*)_([0-9]+)$\", key)\n",
    "    return m.group(1) if m else key\n",
    "\n",
    "def load_xor_training(scenario_full_name: str):\n",
    "    path = f'./data/xai_tris/{scenario_full_name}.pkl'\n",
    "    if not os.path.exists(path): return None, None\n",
    "    try:\n",
    "        data = pkl.load(open(path, \"rb\"))\n",
    "        return data.x_train.float(), data.y_train\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "# =========================\n",
    "# Generic helpers\n",
    "# =========================\n",
    "def reduce_to_vector_per_seed(obj):\n",
    "    if obj is None: return None\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        if len(obj) == 0: return None\n",
    "        if all(np.isscalar(x) or (isinstance(x, np.generic) and np.ndim(x) == 0) for x in obj):\n",
    "            return np.asarray(obj, dtype=float)\n",
    "        arrs = [np.asarray(x) for x in obj]\n",
    "        arrs = [a.squeeze(-1) if (a.ndim == 2 and a.shape[-1] == 1) else a for a in arrs]\n",
    "        if all(a.ndim == 1 for a in arrs):\n",
    "            return np.mean(np.stack(arrs, 0), 0).astype(float)\n",
    "        stacked = np.array([np.ravel(a) for a in arrs], dtype=float)\n",
    "        return np.mean(stacked, 0) if stacked.ndim == 2 else np.ravel(stacked).astype(float)\n",
    "    arr = np.asarray(obj)\n",
    "    if arr.ndim == 1: return arr.astype(float)\n",
    "    if arr.ndim == 2:\n",
    "        if arr.shape[1] == 64: return np.mean(arr, 0).astype(float)\n",
    "        if arr.shape[1] == 1:  return arr[:, 0].astype(float)\n",
    "        if arr.shape[0] == 1:  return arr[0, :].astype(float)\n",
    "        return np.ravel(arr).astype(float)\n",
    "    if arr.ndim == 3 and arr.shape[1] == 64 and arr.shape[2] == 1:\n",
    "        return np.mean(arr[:, :, 0], 0).astype(float)\n",
    "    return np.ravel(arr).astype(float)\n",
    "\n",
    "# =========================\n",
    "# SASI reducers (IMP-SASI)\n",
    "# =========================\n",
    "def _imp_sasi_from_unary_and_pairs(u, pairs_idx, w_pairs, nonneg=True):\n",
    "    u = np.asarray(u, dtype=float)\n",
    "    if nonneg: u = np.abs(u)\n",
    "    sasi = u.copy()\n",
    "    for (i, j), w in zip(pairs_idx, w_pairs):\n",
    "        wv = abs(float(w)) if nonneg else float(w)\n",
    "        if u[i] >= u[j]:\n",
    "            if wv > sasi[i]: sasi[i] = max(u[i], wv)\n",
    "        else:\n",
    "            if wv > sasi[j]: sasi[j] = max(u[j], wv)\n",
    "    return sasi\n",
    "\n",
    "def nam_imp_sasi_from_192(vec192, scenario_key, d=64, nonneg=True):\n",
    "    v = np.asarray(vec192, dtype=float)\n",
    "    if v.size != d + 128:  # fall back to univariate\n",
    "        return np.abs(v[:d]) if nonneg else v[:d]\n",
    "    u = v[:d]; inter = v[d:]\n",
    "    pairs = []\n",
    "    if 'xor' in scenario_key.lower():\n",
    "        Xtr, ytr = load_xor_training(scenario_key)\n",
    "        if Xtr is not None and ytr is not None:\n",
    "            try: pairs, _ = FAST(Xtr, ytr, n_interactions=128)\n",
    "            except Exception: pairs = []\n",
    "    if len(pairs) != 128:\n",
    "        return np.abs(u) if nonneg else u\n",
    "    return _imp_sasi_from_unary_and_pairs(u, pairs, inter, nonneg=nonneg)\n",
    "\n",
    "def qlr_imp_sasi_from_2144(vec2144, d=64, nonneg=True):\n",
    "    v = np.asarray(vec2144, dtype=float)\n",
    "    if v.size != 2144:\n",
    "        return np.abs(v[:d]) if nonneg else v[:d]\n",
    "    u = v[:d]; tri = v[d:]\n",
    "    i0, i1 = np.triu_indices(d, 0)\n",
    "    mask_cross = i0 < i1\n",
    "    pairs = list(zip(i0[mask_cross].tolist(), i1[mask_cross].tolist()))\n",
    "    w_pairs = tri[mask_cross]\n",
    "    return _imp_sasi_from_unary_and_pairs(u, pairs, w_pairs, nonneg=nonneg)\n",
    "\n",
    "# =========================\n",
    "# MAX reducers (credit both ends)\n",
    "# =========================\n",
    "def nam_max_reduce_192_to_64(vec192, scenario_key, d=64):\n",
    "    v = np.asarray(vec192, dtype=float)\n",
    "    if v.size != d + 128:\n",
    "        return np.abs(v[:d])\n",
    "    u = np.abs(v[:d]); inter = np.abs(v[d:])\n",
    "    pairs = []\n",
    "    if 'xor' in scenario_key.lower():\n",
    "        Xtr, ytr = load_xor_training(scenario_key)\n",
    "        if Xtr is not None and ytr is not None:\n",
    "            try: pairs, _ = FAST(Xtr, ytr, n_interactions=128)\n",
    "            except Exception: pairs = []\n",
    "    out = u.copy()\n",
    "    if len(pairs) == 128 and inter.size == 128:\n",
    "        for k,(i,j) in enumerate(pairs):\n",
    "            w = inter[k]\n",
    "            if w > out[i]: out[i] = w\n",
    "            if w > out[j]: out[j] = w\n",
    "    return out\n",
    "\n",
    "def qlr_max_reduce_2144_to_64(vec2144, d=64):\n",
    "    v = np.asarray(vec2144, dtype=float)\n",
    "    if v.size != 2144:\n",
    "        return np.abs(v[:d])\n",
    "    linear = np.abs(v[:d]); tri = np.abs(v[d:])\n",
    "    i0, i1 = np.triu_indices(d, 0)\n",
    "    out = linear.copy()\n",
    "    for k,(a,b) in enumerate(zip(i0,i1)):\n",
    "        w = tri[k]\n",
    "        if w > out[a]: out[a] = w\n",
    "        if a != b and w > out[b]: out[b] = w\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Dispatchers\n",
    "# =========================\n",
    "def primary_to_64(expl_raw, method: str, scenario_key: str):\n",
    "    \"\"\"PAT/EBM: choose SASI vs MAX based on switch; others handled elsewhere.\"\"\"\n",
    "    v = reduce_to_vector_per_seed(expl_raw)\n",
    "    if v is None: return np.full(64, np.nan, dtype=float)\n",
    "    n = v.size\n",
    "    use_sasi = USE_SASI_FOR_PRIMARY\n",
    "\n",
    "    if method == 'pattern_gam' or method == 'ebm':\n",
    "        if n == 64:   return np.abs(v)\n",
    "        if n == 192:  return (nam_imp_sasi_from_192(v, scenario_key) if use_sasi\n",
    "                              else nam_max_reduce_192_to_64(v, scenario_key))\n",
    "        return np.abs(v[:64])\n",
    "\n",
    "    if method == 'pattern_qlr':\n",
    "        if n == 64:    return np.abs(v)\n",
    "        if n == 2144:  return (qlr_imp_sasi_from_2144(v) if use_sasi\n",
    "                               else qlr_max_reduce_2144_to_64(v))\n",
    "        return np.abs(v[:64])\n",
    "\n",
    "    return np.full(64, np.nan, dtype=float)\n",
    "\n",
    "def tilde_to_64(vec, family_tag, scenario_key):\n",
    "    \"\"\"SD/SDb/DISCR/PROD vectors for NAM/QLR (SASI vs MAX per switch).\"\"\"\n",
    "    v = np.asarray(vec)\n",
    "    if v.ndim != 1: v = np.ravel(v)\n",
    "    n = v.size\n",
    "    use_sasi = USE_SASI_FOR_TILDE\n",
    "\n",
    "    if family_tag == 'NAM':\n",
    "        if n == 64:  return np.abs(v)\n",
    "        if n == 192: return nam_imp_sasi_from_192(v, scenario_key) if use_sasi else nam_max_reduce_192_to_64(v, scenario_key)\n",
    "        return np.abs(v[:64])\n",
    "\n",
    "    if family_tag == 'QLR':\n",
    "        if n == 64:    return np.abs(v)\n",
    "        if n == 2144:  return qlr_imp_sasi_from_2144(v) if use_sasi else qlr_max_reduce_2144_to_64(v)\n",
    "        return np.abs(v[:64])\n",
    "\n",
    "    return np.abs(v[:64])\n",
    "\n",
    "# =========================\n",
    "# Metric computation adapter\n",
    "# =========================\n",
    "def compute_metrics(vec64):\n",
    "    if vec64 is None or vec64.size != 64:\n",
    "        return {'IMA': np.nan, 'EMD': np.nan, 'FNI_EMD': np.nan}\n",
    "    return {\n",
    "        'IMA': importance_mass_accuracy(GT_MASK_2D_FLAT, vec64),\n",
    "        'EMD': calculate_emd_score_metric(GT_MASK_2D_FLAT, vec64, D_2D_EDGE, COST_MATRIX_MAIN_EFFECTS, is_fni=False),\n",
    "        'FNI_EMD': calculate_emd_score_metric(GT_MASK_2D_FLAT, vec64, D_2D_EDGE, COST_MATRIX_MAIN_EFFECTS, is_fni=True),\n",
    "    }\n",
    "\n",
    "def nanmeanstd(xs):\n",
    "    if not xs: return (np.nan, np.nan)\n",
    "    arr = np.array(xs, dtype=float)\n",
    "    return np.nanmean(arr), np.nanstd(arr)\n",
    "\n",
    "# =========================\n",
    "# Evaluate all methods\n",
    "# =========================\n",
    "EVAL_METHODS = ['pattern_gam', 'pattern_qlr', 'kernel_svm', 'ebm', 'shap', 'ig', 'pattern_net', 'pattern_attribution']\n",
    "\n",
    "results_raw_collection = {}   # {sc_base: {method: {'IMA':[], 'EMD':[], 'FNI_EMD':[]}}}\n",
    "final_aggregated_results = {}\n",
    "\n",
    "# ---- Primary explanations (PAT/EBM use SASI or MAX per switch) ----\n",
    "for scenario_key, method_dict in explanations.items():\n",
    "    sc_base = base_scenario_name(scenario_key)\n",
    "    results_raw_collection.setdefault(sc_base, {})\n",
    "\n",
    "    for method in EVAL_METHODS:\n",
    "        if method not in method_dict: continue\n",
    "        payloads = method_dict[method]  # list of seeds\n",
    "\n",
    "        ml = results_raw_collection[sc_base].setdefault(method, {'IMA': [], 'EMD': [], 'FNI_EMD': []})\n",
    "        for seed_payload in payloads:\n",
    "            if method in ('pattern_gam', 'pattern_qlr', 'ebm'):\n",
    "                vec64 = primary_to_64(seed_payload, method=method, scenario_key=scenario_key)\n",
    "            elif method in ('shap', 'ig', 'pattern_net', 'pattern_attribution'):\n",
    "                v = reduce_to_vector_per_seed(seed_payload)\n",
    "                if v is None: vec64 = np.full(64, np.nan)\n",
    "                else:\n",
    "                    # Globalize locals by mean |·| if needed\n",
    "                    if v.size == 64: vec64 = np.abs(v)\n",
    "                    else:\n",
    "                        # try to interpret as (B,64) or reduced already by reduce_to_vector_per_seed\n",
    "                        vec64 = np.abs(v[:64]) if v.size >= 64 else np.full(64, np.nan)\n",
    "            elif method == 'kernel_svm':\n",
    "                v = reduce_to_vector_per_seed(seed_payload)\n",
    "                vec64 = np.abs(v) if (v is not None and v.size == 64) else np.full(64, np.nan)\n",
    "            else:\n",
    "                vec64 = np.full(64, np.nan)\n",
    "\n",
    "            mets = compute_metrics(vec64)\n",
    "            ml['IMA'].append(mets['IMA']); ml['EMD'].append(mets['EMD']); ml['FNI_EMD'].append(mets['FNI_EMD'])\n",
    "\n",
    "# ---- Tilde metrics (NAM & QLR): SD / SDb / DISCR / PROD; SASI or MAX per switch ----\n",
    "TILDE_METHOD_LABELS = {\n",
    "    'NAM': {'SD_tilde': 'SD_NAM', 'SDb_tilde': 'SDb_NAM', 'DISCR_tilde': 'DISCR_NAM'},\n",
    "    'QLR': {'SD_tilde': 'SD_QLR', 'SDb_tilde': 'SDb_QLR', 'DISCR_tilde': 'DISCR_QLR'},\n",
    "}\n",
    "\n",
    "for scenario_key, blocks in explanations_nam.items():\n",
    "    sc_base = base_scenario_name(scenario_key)\n",
    "    results_raw_collection.setdefault(sc_base, {})\n",
    "\n",
    "    # NAM tilde\n",
    "    nam_list = blocks.get('nam_tilde_metrics', [])\n",
    "    for entry in nam_list:\n",
    "        for k_src, k_dst in TILDE_METHOD_LABELS['NAM'].items():\n",
    "            if k_src not in entry: continue\n",
    "            vec64 = tilde_to_64(entry[k_src], family_tag='NAM', scenario_key=scenario_key)\n",
    "            mets = compute_metrics(vec64)\n",
    "            ml = results_raw_collection[sc_base].setdefault(k_dst, {'IMA': [], 'EMD': [], 'FNI_EMD': []})\n",
    "            ml['IMA'].append(mets['IMA']); ml['EMD'].append(mets['EMD']); ml['FNI_EMD'].append(mets['FNI_EMD'])\n",
    "        sd_vec = entry.get('SD_tilde', None); sdb_vec = entry.get('SDb_tilde', None)\n",
    "        if sd_vec is not None and sdb_vec is not None:\n",
    "            prod_vec = np.asarray(sd_vec, float) * np.asarray(sdb_vec, float)\n",
    "            vec64 = tilde_to_64(prod_vec, family_tag='NAM', scenario_key=scenario_key)\n",
    "            mets = compute_metrics(vec64)\n",
    "            ml = results_raw_collection[sc_base].setdefault('PROD_NAM', {'IMA': [], 'EMD': [], 'FNI_EMD': []})\n",
    "            ml['IMA'].append(mets['IMA']); ml['EMD'].append(mets['EMD']); ml['FNI_EMD'].append(mets['FNI_EMD'])\n",
    "\n",
    "    # QLR tilde\n",
    "    qlr_list = blocks.get('qlr_tilde_metrics', [])\n",
    "    for entry in qlr_list:\n",
    "        for k_src, k_dst in TILDE_METHOD_LABELS['QLR'].items():\n",
    "            if k_src not in entry: continue\n",
    "            vec64 = tilde_to_64(entry[k_src], family_tag='QLR', scenario_key=scenario_key)\n",
    "            mets = compute_metrics(vec64)\n",
    "            ml = results_raw_collection[sc_base].setdefault(k_dst, {'IMA': [], 'EMD': [], 'FNI_EMD': []})\n",
    "            ml['IMA'].append(mets['IMA']); ml['EMD'].append(mets['EMD']); ml['FNI_EMD'].append(mets['FNI_EMD'])\n",
    "        sd_vec = entry.get('SD_tilde', None); sdb_vec = entry.get('SDb_tilde', None)\n",
    "        if sd_vec is not None and sdb_vec is not None:\n",
    "            prod_vec = np.asarray(sd_vec, float) * np.asarray(sdb_vec, float)\n",
    "            vec64 = tilde_to_64(prod_vec, family_tag='QLR', scenario_key=scenario_key)\n",
    "            mets = compute_metrics(vec64)\n",
    "            ml = results_raw_collection[sc_base].setdefault('PROD_QLR', {'IMA': [], 'EMD': [], 'FNI_EMD': []})\n",
    "            ml['IMA'].append(mets['IMA']); ml['EMD'].append(mets['EMD']); ml['FNI_EMD'].append(mets['FNI_EMD'])\n",
    "\n",
    "# ---- Aggregate ----\n",
    "final_aggregated_results = {}\n",
    "for sc_base, meth_data in results_raw_collection.items():\n",
    "    final_aggregated_results[sc_base] = {}\n",
    "    for meth, ml in meth_data.items():\n",
    "        IMA_m, IMA_s = nanmeanstd(ml['IMA'])\n",
    "        EMD_m, EMD_s = nanmeanstd(ml['EMD'])\n",
    "        FNI_m, FNI_s = nanmeanstd(ml['FNI_EMD'])\n",
    "        final_aggregated_results[sc_base][meth] = {\n",
    "            'IMA_mean': IMA_m, 'IMA_std': IMA_s,\n",
    "            'EMD_mean': EMD_m, 'EMD_std': EMD_s,\n",
    "            'FNI_EMD_mean': FNI_m, 'FNI_EMD_std': FNI_s,\n",
    "        }\n",
    "\n",
    "\n",
    "# =========================\n",
    "#      Print summary \n",
    "# =========================\n",
    "def brief_summary(d):\n",
    "    lines = []\n",
    "    for sc in sorted(d.keys()):\n",
    "        lines.append(f\"[{sc}]\")\n",
    "        for meth, vals in sorted(d[sc].items()):\n",
    "            lines.append(\n",
    "                f\"  {meth}: IMA={vals['IMA_mean']:.3f}±{vals['IMA_std']:.3f}, \"\n",
    "                f\"EMD={vals['EMD_mean']:.3f}±{vals['EMD_std']:.3f}, \"\n",
    "                f\"FNI-EMD={vals['FNI_EMD_mean']:.3f}±{vals['FNI_EMD_std']:.3f}\"\n",
    "            )\n",
    "        lines.append(\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(\"=== Aggregated results (means ± stds) ===\")\n",
    "print(brief_summary(final_aggregated_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f684f794-fbb0-4f6a-b221-42227f18b43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "% --- LaTeX Table for IMA (transposed, MULT banded) ---\n",
      "% Transposed LaTeX Table for IMA (MULT banded)\n",
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\caption{Importance Mass Accuracy (IMA). Values are mean $\\pm$ standard deviation. Best result per column (ties at 2 dp) is emboldened.}\n",
      "\\label{tab:ima_results_transposed}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\begin{tabular}{l|ccccMMMMcccc}\n",
      "\\hline\n",
      " & \\multicolumn{4}{c}{LIN} & \\multicolumn{4}{c}{\\cellcolor{colMULT} MULT} & \\multicolumn{4}{c}{XOR} \\\\\n",
      "Method & WHITE & CORR & DIST WHITE & DIST CORR & WHITE & CORR & DIST WHITE & DIST CORR & WHITE & CORR & DIST WHITE & DIST CORR \\\\ \\hline\\hline\n",
      "$\\text{PAT}^{\\text{GAM}}$ & $0.86 \\pm 0.02$ & $0.40 \\pm 0.10$ & $0.89 \\pm 0.01$ & $0.36 \\pm 0.11$ & $0.81 \\pm 0.04$ & $0.36 \\pm 0.11$ & $0.82 \\pm 0.03$ & $0.38 \\pm 0.11$ & $0.19 \\pm 0.05$ & $0.26 \\pm 0.09$ & $0.30 \\pm 0.09$ & $0.24 \\pm 0.10$ \\\\\n",
      "$\\text{PAT}^{\\text{QLR}}$ & $0.70 \\pm 0.01$ & $0.52 \\pm 0.07$ & $0.74 \\pm 0.00$ & $0.37 \\pm 0.04$ & $0.16 \\pm 0.02$ & $0.21 \\pm 0.02$ & $0.16 \\pm 0.02$ & $0.22 \\pm 0.04$ & $0.74 \\pm 0.02$ & $0.59 \\pm 0.05$ & $0.67 \\pm 0.05$ & $0.60 \\pm 0.09$ \\\\\n",
      "$\\text{SD}(f^{\\text{GAM}})$ & $0.93 \\pm 0.04$ & $0.48 \\pm 0.05$ & $0.76 \\pm 0.04$ & $0.46 \\pm 0.04$ & $0.90 \\pm 0.03$ & $0.46 \\pm 0.05$ & $0.90 \\pm 0.04$ & $0.48 \\pm 0.05$ & $0.69 \\pm 0.12$ & $0.53 \\pm 0.16$ & $0.65 \\pm 0.13$ & $0.53 \\pm 0.19$ \\\\\n",
      "$\\text{SD}(f^{\\text{PGAM}})$ & $0.97 \\pm 0.02$ & $0.85 \\pm 0.06$ & $0.99 \\pm 0.01$ & $0.78 \\pm 0.07$ & $0.94 \\pm 0.03$ & $0.84 \\pm 0.07$ & $0.94 \\pm 0.03$ & $0.85 \\pm 0.07$ & $0.23 \\pm 0.09$ & $0.20 \\pm 0.09$ & $0.24 \\pm 0.11$ & $0.25 \\pm 0.11$ \\\\\n",
      "$\\text{PROD}(f^{\\text{GAM}})$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{0.96 \\pm 0.03}$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{0.94 \\pm 0.03}$ & $\\mathbf{0.99 \\pm 0.01}$ & $\\mathbf{0.95 \\pm 0.03}$ & $\\mathbf{0.99 \\pm 0.01}$ & $\\mathbf{0.95 \\pm 0.03}$ & $0.59 \\pm 0.17$ & $0.44 \\pm 0.18$ & $0.55 \\pm 0.19$ & $0.48 \\pm 0.23$ \\\\\n",
      "$\\text{DISCR}(f^{\\text{GAM}})$ & $0.90 \\pm 0.01$ & $0.80 \\pm 0.07$ & $0.91 \\pm 0.01$ & $0.71 \\pm 0.07$ & $0.74 \\pm 0.03$ & $0.64 \\pm 0.10$ & $0.75 \\pm 0.03$ & $0.64 \\pm 0.10$ & $0.10 \\pm 0.03$ & $0.12 \\pm 0.04$ & $0.11 \\pm 0.03$ & $0.13 \\pm 0.05$ \\\\\n",
      "$\\text{SD}(f^{\\text{QLR}})$ & $0.59 \\pm 0.01$ & $0.49 \\pm 0.00$ & $0.55 \\pm 0.02$ & $0.49 \\pm 0.00$ & $0.12 \\pm 0.02$ & $0.25 \\pm 0.04$ & $0.12 \\pm 0.02$ & $0.24 \\pm 0.03$ & $0.61 \\pm 0.04$ & $0.68 \\pm 0.08$ & $0.51 \\pm 0.05$ & $0.64 \\pm 0.05$ \\\\\n",
      "$\\text{SD}(f^{\\text{PQLR}})$ & $0.78 \\pm 0.00$ & $0.80 \\pm 0.10$ & $0.86 \\pm 0.00$ & $0.66 \\pm 0.08$ & $0.12 \\pm 0.01$ & $0.33 \\pm 0.11$ & $0.12 \\pm 0.01$ & $0.29 \\pm 0.03$ & $0.88 \\pm 0.01$ & $0.93 \\pm 0.04$ & $0.81 \\pm 0.03$ & $0.89 \\pm 0.05$ \\\\\n",
      "$\\text{PROD}(f^{\\text{QLR}})$ & $0.98 \\pm 0.00$ & $\\mathbf{0.96 \\pm 0.02}$ & $0.99 \\pm 0.00$ & $0.93 \\pm 0.02$ & $0.11 \\pm 0.02$ & $0.48 \\pm 0.06$ & $0.11 \\pm 0.02$ & $0.47 \\pm 0.08$ & $\\mathbf{0.99 \\pm 0.00}$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{0.99 \\pm 0.00}$ & $\\mathbf{0.99 \\pm 0.00}$ \\\\\n",
      "$\\text{DISCR}(f^{\\text{QLR}})$ & $0.74 \\pm 0.00$ & $0.66 \\pm 0.05$ & $0.77 \\pm 0.01$ & $0.56 \\pm 0.05$ & $0.12 \\pm 0.01$ & $0.30 \\pm 0.11$ & $0.12 \\pm 0.01$ & $0.30 \\pm 0.10$ & $0.76 \\pm 0.02$ & $0.77 \\pm 0.03$ & $0.68 \\pm 0.04$ & $0.72 \\pm 0.06$ \\\\\n",
      "EBM & $0.88 \\pm 0.01$ & $0.57 \\pm 0.00$ & $0.63 \\pm 0.01$ & $0.52 \\pm 0.00$ & $0.88 \\pm 0.01$ & $0.47 \\pm 0.00$ & $0.87 \\pm 0.01$ & $0.46 \\pm 0.01$ & $0.09 \\pm 0.02$ & $0.13 \\pm 0.05$ & $0.09 \\pm 0.02$ & $0.14 \\pm 0.03$ \\\\\n",
      "Kernel Pattern & $0.89 \\pm 0.01$ & $0.80 \\pm 0.07$ & $0.93 \\pm 0.01$ & $0.74 \\pm 0.06$ & $0.09 \\pm 0.02$ & $0.09 \\pm 0.03$ & $0.09 \\pm 0.02$ & $0.09 \\pm 0.03$ & $0.12 \\pm 0.03$ & $0.11 \\pm 0.03$ & $0.16 \\pm 0.04$ & $0.14 \\pm 0.05$ \\\\\n",
      "PatternNet & $0.71 \\pm 0.16$ & $0.40 \\pm 0.19$ & $0.80 \\pm 0.11$ & $0.32 \\pm 0.14$ & $0.32 \\pm 0.09$ & $0.11 \\pm 0.03$ & $0.34 \\pm 0.09$ & $0.12 \\pm 0.04$ & $0.75 \\pm 0.15$ & $0.55 \\pm 0.20$ & $0.74 \\pm 0.14$ & $0.56 \\pm 0.21$ \\\\\n",
      "PatternAttribution & $0.96 \\pm 0.05$ & $0.42 \\pm 0.17$ & $0.93 \\pm 0.08$ & $0.37 \\pm 0.13$ & $0.85 \\pm 0.12$ & $0.37 \\pm 0.12$ & $0.86 \\pm 0.10$ & $0.39 \\pm 0.12$ & $0.98 \\pm 0.02$ & $0.78 \\pm 0.19$ & $0.93 \\pm 0.06$ & $0.82 \\pm 0.11$ \\\\\n",
      "SHAP & $0.74 \\pm 0.06$ & $0.45 \\pm 0.04$ & $0.54 \\pm 0.09$ & $0.43 \\pm 0.03$ & $0.65 \\pm 0.07$ & $0.39 \\pm 0.10$ & $0.64 \\pm 0.07$ & $0.39 \\pm 0.08$ & $0.60 \\pm 0.09$ & $0.37 \\pm 0.09$ & $0.49 \\pm 0.07$ & $0.38 \\pm 0.05$ \\\\\n",
      "Int. Grads. & $0.97 \\pm 0.01$ & $0.86 \\pm 0.04$ & $0.95 \\pm 0.01$ & $0.80 \\pm 0.05$ & $0.93 \\pm 0.02$ & $0.48 \\pm 0.00$ & $0.93 \\pm 0.02$ & $0.48 \\pm 0.01$ & $0.82 \\pm 0.08$ & $0.76 \\pm 0.12$ & $0.67 \\pm 0.11$ & $0.77 \\pm 0.12$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "}\n",
      "\\end{table}\n",
      "% --- End of LaTeX Table ---\n",
      "\n",
      "% --- LaTeX Table for EMD (transposed, MULT banded) ---\n",
      "% Transposed LaTeX Table for EMD (MULT banded)\n",
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\caption{Earth Mover's Distance (EMD). Values are mean $\\pm$ standard deviation. Best result per column (ties at 2 dp) is emboldened.}\n",
      "\\label{tab:emd_results_transposed}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\begin{tabular}{l|ccccMMMMcccc}\n",
      "\\hline\n",
      " & \\multicolumn{4}{c}{LIN} & \\multicolumn{4}{c}{\\cellcolor{colMULT} MULT} & \\multicolumn{4}{c}{XOR} \\\\\n",
      "Method & WHITE & CORR & DIST WHITE & DIST CORR & WHITE & CORR & DIST WHITE & DIST CORR & WHITE & CORR & DIST WHITE & DIST CORR \\\\ \\hline\\hline\n",
      "$\\text{PAT}^{\\text{GAM}}$ & $0.94 \\pm 0.01$ & $0.78 \\pm 0.03$ & $0.92 \\pm 0.02$ & $0.77 \\pm 0.05$ & $0.91 \\pm 0.04$ & $0.76 \\pm 0.04$ & $0.91 \\pm 0.03$ & $0.76 \\pm 0.05$ & $0.70 \\pm 0.03$ & $0.73 \\pm 0.04$ & $0.74 \\pm 0.04$ & $0.72 \\pm 0.05$ \\\\\n",
      "$\\text{PAT}^{\\text{QLR}}$ & $0.89 \\pm 0.00$ & $0.82 \\pm 0.03$ & $0.89 \\pm 0.00$ & $0.77 \\pm 0.02$ & $0.69 \\pm 0.01$ & $0.73 \\pm 0.02$ & $0.69 \\pm 0.01$ & $0.73 \\pm 0.03$ & $0.87 \\pm 0.04$ & $0.78 \\pm 0.06$ & $0.79 \\pm 0.07$ & $0.79 \\pm 0.06$ \\\\\n",
      "$\\text{SD}(f^{\\text{GAM}})$ & $0.96 \\pm 0.02$ & $0.79 \\pm 0.07$ & $0.89 \\pm 0.04$ & $0.71 \\pm 0.11$ & $0.93 \\pm 0.03$ & $0.81 \\pm 0.04$ & $0.93 \\pm 0.04$ & $0.78 \\pm 0.06$ & $0.83 \\pm 0.05$ & $0.75 \\pm 0.07$ & $0.80 \\pm 0.05$ & $0.75 \\pm 0.07$ \\\\\n",
      "$\\text{SD}(f^{\\text{PGAM}})$ & $0.97 \\pm 0.02$ & $0.93 \\pm 0.03$ & $0.89 \\pm 0.03$ & $\\mathbf{0.89 \\pm 0.03}$ & $0.94 \\pm 0.03$ & $\\mathbf{0.91 \\pm 0.03}$ & $0.93 \\pm 0.04$ & $\\mathbf{0.91 \\pm 0.03}$ & $0.68 \\pm 0.06$ & $0.68 \\pm 0.06$ & $0.68 \\pm 0.07$ & $0.69 \\pm 0.07$ \\\\\n",
      "$\\text{PROD}(f^{\\text{GAM}})$ & $0.96 \\pm 0.03$ & $0.84 \\pm 0.07$ & $0.82 \\pm 0.05$ & $0.73 \\pm 0.12$ & $0.91 \\pm 0.06$ & $0.85 \\pm 0.05$ & $0.91 \\pm 0.05$ & $0.82 \\pm 0.08$ & $0.76 \\pm 0.08$ & $0.70 \\pm 0.09$ & $0.74 \\pm 0.08$ & $0.70 \\pm 0.10$ \\\\\n",
      "$\\text{DISCR}(f^{\\text{GAM}})$ & $0.95 \\pm 0.02$ & $0.91 \\pm 0.03$ & $0.89 \\pm 0.02$ & $0.86 \\pm 0.03$ & $0.89 \\pm 0.02$ & $0.85 \\pm 0.04$ & $0.89 \\pm 0.02$ & $0.85 \\pm 0.03$ & $0.66 \\pm 0.03$ & $0.66 \\pm 0.04$ & $0.66 \\pm 0.03$ & $0.66 \\pm 0.03$ \\\\\n",
      "$\\text{SD}(f^{\\text{QLR}})$ & $0.85 \\pm 0.01$ & $0.86 \\pm 0.00$ & $0.86 \\pm 0.01$ & $0.83 \\pm 0.00$ & $0.68 \\pm 0.01$ & $0.75 \\pm 0.02$ & $0.68 \\pm 0.01$ & $0.74 \\pm 0.02$ & $0.84 \\pm 0.02$ & $0.75 \\pm 0.09$ & $0.81 \\pm 0.02$ & $0.82 \\pm 0.05$ \\\\\n",
      "$\\text{SD}(f^{\\text{PQLR}})$ & $0.92 \\pm 0.00$ & $0.92 \\pm 0.04$ & $0.83 \\pm 0.00$ & $0.86 \\pm 0.02$ & $0.68 \\pm 0.01$ & $0.76 \\pm 0.04$ & $0.68 \\pm 0.01$ & $0.75 \\pm 0.03$ & $0.86 \\pm 0.05$ & $0.68 \\pm 0.13$ & $0.78 \\pm 0.09$ & $0.77 \\pm 0.10$ \\\\\n",
      "$\\text{PROD}(f^{\\text{QLR}})$ & $\\mathbf{0.98 \\pm 0.01}$ & $\\mathbf{0.97 \\pm 0.02}$ & $0.82 \\pm 0.01$ & $0.85 \\pm 0.02$ & $0.67 \\pm 0.01$ & $0.82 \\pm 0.02$ & $0.67 \\pm 0.01$ & $0.82 \\pm 0.03$ & $\\mathbf{0.88 \\pm 0.04}$ & $0.65 \\pm 0.14$ & $0.71 \\pm 0.13$ & $0.72 \\pm 0.10$ \\\\\n",
      "$\\text{DISCR}(f^{\\text{QLR}})$ & $0.91 \\pm 0.00$ & $0.87 \\pm 0.02$ & $0.90 \\pm 0.00$ & $0.83 \\pm 0.02$ & $0.68 \\pm 0.01$ & $0.75 \\pm 0.03$ & $0.68 \\pm 0.01$ & $0.75 \\pm 0.03$ & $0.86 \\pm 0.04$ & $0.72 \\pm 0.11$ & $0.80 \\pm 0.05$ & $0.77 \\pm 0.08$ \\\\\n",
      "EBM & $0.96 \\pm 0.00$ & $0.84 \\pm 0.01$ & $0.90 \\pm 0.01$ & $0.79 \\pm 0.01$ & $\\mathbf{0.96 \\pm 0.00}$ & $0.88 \\pm 0.00$ & $\\mathbf{0.95 \\pm 0.00}$ & $0.87 \\pm 0.00$ & $0.66 \\pm 0.01$ & $0.68 \\pm 0.03$ & $0.66 \\pm 0.01$ & $0.67 \\pm 0.02$ \\\\\n",
      "Kernel Pattern & $0.96 \\pm 0.00$ & $0.92 \\pm 0.03$ & $\\mathbf{0.97 \\pm 0.00}$ & $\\mathbf{0.89 \\pm 0.02}$ & $0.67 \\pm 0.02$ & $0.61 \\pm 0.04$ & $0.67 \\pm 0.02$ & $0.61 \\pm 0.05$ & $0.68 \\pm 0.02$ & $0.62 \\pm 0.04$ & $0.68 \\pm 0.03$ & $0.63 \\pm 0.05$ \\\\\n",
      "PatternNet & $0.89 \\pm 0.06$ & $0.78 \\pm 0.07$ & $0.90 \\pm 0.06$ & $0.74 \\pm 0.05$ & $0.73 \\pm 0.03$ & $0.68 \\pm 0.01$ & $0.74 \\pm 0.03$ & $0.68 \\pm 0.02$ & $0.85 \\pm 0.08$ & $0.79 \\pm 0.08$ & $0.85 \\pm 0.07$ & $0.80 \\pm 0.08$ \\\\\n",
      "PatternAttribution & $0.94 \\pm 0.05$ & $0.75 \\pm 0.06$ & $0.86 \\pm 0.07$ & $0.75 \\pm 0.07$ & $0.77 \\pm 0.10$ & $0.78 \\pm 0.05$ & $0.76 \\pm 0.09$ & $0.77 \\pm 0.06$ & $0.86 \\pm 0.08$ & $0.82 \\pm 0.09$ & $\\mathbf{0.87 \\pm 0.07}$ & $0.82 \\pm 0.08$ \\\\\n",
      "SHAP & $0.84 \\pm 0.04$ & $0.76 \\pm 0.07$ & $0.82 \\pm 0.04$ & $0.77 \\pm 0.10$ & $0.82 \\pm 0.04$ & $0.78 \\pm 0.05$ & $0.83 \\pm 0.03$ & $0.76 \\pm 0.05$ & $0.83 \\pm 0.03$ & $0.75 \\pm 0.05$ & $0.80 \\pm 0.04$ & $0.76 \\pm 0.04$ \\\\\n",
      "Int. Grads. & $0.96 \\pm 0.04$ & $0.94 \\pm 0.02$ & $0.84 \\pm 0.05$ & $0.81 \\pm 0.04$ & $0.92 \\pm 0.05$ & $0.86 \\pm 0.01$ & $0.92 \\pm 0.05$ & $0.82 \\pm 0.02$ & $0.86 \\pm 0.05$ & $\\mathbf{0.88 \\pm 0.06}$ & $0.84 \\pm 0.05$ & $\\mathbf{0.86 \\pm 0.05}$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "}\n",
      "\\end{table}\n",
      "% --- End of LaTeX Table ---\n",
      "\n",
      "% --- LaTeX Table for FNI-EMD (transposed, MULT banded) ---\n",
      "% Transposed LaTeX Table for FNI-EMD (MULT banded)\n",
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\caption{False-Negative Invariant EMD (FNI-EMD). Values are mean $\\pm$ standard deviation. Best result per column (ties at 2 dp) is emboldened.}\n",
      "\\label{tab:fniemd_results_transposed}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\begin{tabular}{l|ccccMMMMcccc}\n",
      "\\hline\n",
      " & \\multicolumn{4}{c}{LIN} & \\multicolumn{4}{c}{\\cellcolor{colMULT} MULT} & \\multicolumn{4}{c}{XOR} \\\\\n",
      "Method & WHITE & CORR & DIST WHITE & DIST CORR & WHITE & CORR & DIST WHITE & DIST CORR & WHITE & CORR & DIST WHITE & DIST CORR \\\\ \\hline\\hline\n",
      "$\\text{PAT}^{\\text{GAM}}$ & $0.95 \\pm 0.01$ & $0.80 \\pm 0.04$ & $0.96 \\pm 0.01$ & $0.80 \\pm 0.04$ & $0.94 \\pm 0.01$ & $0.80 \\pm 0.04$ & $0.94 \\pm 0.01$ & $0.81 \\pm 0.04$ & $0.71 \\pm 0.03$ & $0.75 \\pm 0.04$ & $0.76 \\pm 0.04$ & $0.74 \\pm 0.05$ \\\\\n",
      "$\\text{PAT}^{\\text{QLR}}$ & $0.90 \\pm 0.00$ & $0.84 \\pm 0.03$ & $0.91 \\pm 0.00$ & $0.79 \\pm 0.02$ & $0.70 \\pm 0.01$ & $0.74 \\pm 0.02$ & $0.70 \\pm 0.01$ & $0.75 \\pm 0.03$ & $0.91 \\pm 0.01$ & $0.87 \\pm 0.02$ & $0.89 \\pm 0.02$ & $0.88 \\pm 0.04$ \\\\\n",
      "$\\text{SD}(f^{\\text{GAM}})$ & $0.98 \\pm 0.01$ & $0.89 \\pm 0.02$ & $0.96 \\pm 0.01$ & $0.88 \\pm 0.02$ & $0.97 \\pm 0.01$ & $0.89 \\pm 0.02$ & $0.97 \\pm 0.02$ & $0.89 \\pm 0.01$ & $0.90 \\pm 0.04$ & $0.85 \\pm 0.07$ & $0.88 \\pm 0.05$ & $0.86 \\pm 0.07$ \\\\\n",
      "$\\text{SD}(f^{\\text{PGAM}})$ & $0.99 \\pm 0.01$ & $0.96 \\pm 0.02$ & $\\mathbf{1.00 \\pm 0.00}$ & $0.94 \\pm 0.02$ & $0.98 \\pm 0.01$ & $0.96 \\pm 0.02$ & $0.98 \\pm 0.01$ & $0.96 \\pm 0.02$ & $0.71 \\pm 0.06$ & $0.71 \\pm 0.06$ & $0.71 \\pm 0.07$ & $0.75 \\pm 0.07$ \\\\\n",
      "$\\text{PROD}(f^{\\text{GAM}})$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{0.99 \\pm 0.01}$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{0.99 \\pm 0.01}$ & $\\mathbf{1.00 \\pm 0.01}$ & $\\mathbf{0.99 \\pm 0.01}$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{0.99 \\pm 0.01}$ & $0.85 \\pm 0.08$ & $0.81 \\pm 0.09$ & $0.83 \\pm 0.09$ & $0.83 \\pm 0.10$ \\\\\n",
      "$\\text{DISCR}(f^{\\text{GAM}})$ & $0.97 \\pm 0.01$ & $0.93 \\pm 0.02$ & $0.97 \\pm 0.01$ & $0.90 \\pm 0.02$ & $0.91 \\pm 0.01$ & $0.88 \\pm 0.03$ & $0.91 \\pm 0.01$ & $0.88 \\pm 0.03$ & $0.67 \\pm 0.03$ & $0.67 \\pm 0.04$ & $0.67 \\pm 0.03$ & $0.68 \\pm 0.04$ \\\\\n",
      "$\\text{SD}(f^{\\text{QLR}})$ & $0.86 \\pm 0.00$ & $0.87 \\pm 0.00$ & $0.88 \\pm 0.01$ & $0.89 \\pm 0.00$ & $0.69 \\pm 0.01$ & $0.76 \\pm 0.02$ & $0.69 \\pm 0.01$ & $0.75 \\pm 0.02$ & $0.87 \\pm 0.01$ & $0.91 \\pm 0.03$ & $0.85 \\pm 0.01$ & $0.91 \\pm 0.01$ \\\\\n",
      "$\\text{SD}(f^{\\text{PQLR}})$ & $0.93 \\pm 0.00$ & $0.94 \\pm 0.03$ & $0.95 \\pm 0.00$ & $0.89 \\pm 0.03$ & $0.69 \\pm 0.01$ & $0.78 \\pm 0.04$ & $0.69 \\pm 0.01$ & $0.77 \\pm 0.03$ & $0.96 \\pm 0.00$ & $0.98 \\pm 0.01$ & $0.94 \\pm 0.01$ & $0.96 \\pm 0.02$ \\\\\n",
      "$\\text{PROD}(f^{\\text{QLR}})$ & $0.99 \\pm 0.00$ & $\\mathbf{0.99 \\pm 0.01}$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{0.99 \\pm 0.00}$ & $0.68 \\pm 0.01$ & $0.84 \\pm 0.02$ & $0.68 \\pm 0.01$ & $0.83 \\pm 0.03$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{1.00 \\pm 0.00}$ & $\\mathbf{1.00 \\pm 0.00}$ \\\\\n",
      "$\\text{DISCR}(f^{\\text{QLR}})$ & $0.91 \\pm 0.00$ & $0.89 \\pm 0.02$ & $0.92 \\pm 0.00$ & $0.85 \\pm 0.02$ & $0.69 \\pm 0.01$ & $0.77 \\pm 0.04$ & $0.69 \\pm 0.01$ & $0.77 \\pm 0.03$ & $0.92 \\pm 0.01$ & $0.92 \\pm 0.01$ & $0.89 \\pm 0.01$ & $0.90 \\pm 0.02$ \\\\\n",
      "EBM & $0.96 \\pm 0.00$ & $0.90 \\pm 0.00$ & $0.92 \\pm 0.00$ & $0.91 \\pm 0.00$ & $0.96 \\pm 0.00$ & $0.90 \\pm 0.00$ & $0.96 \\pm 0.00$ & $0.89 \\pm 0.00$ & $0.67 \\pm 0.01$ & $0.69 \\pm 0.03$ & $0.67 \\pm 0.01$ & $0.68 \\pm 0.02$ \\\\\n",
      "Kernel Pattern & $0.96 \\pm 0.00$ & $0.93 \\pm 0.02$ & $0.98 \\pm 0.00$ & $0.91 \\pm 0.02$ & $0.68 \\pm 0.02$ & $0.63 \\pm 0.05$ & $0.68 \\pm 0.02$ & $0.62 \\pm 0.05$ & $0.69 \\pm 0.02$ & $0.64 \\pm 0.05$ & $0.69 \\pm 0.03$ & $0.65 \\pm 0.06$ \\\\\n",
      "PatternNet & $0.90 \\pm 0.05$ & $0.79 \\pm 0.07$ & $0.94 \\pm 0.03$ & $0.76 \\pm 0.05$ & $0.76 \\pm 0.04$ & $0.69 \\pm 0.01$ & $0.77 \\pm 0.04$ & $0.70 \\pm 0.02$ & $0.91 \\pm 0.05$ & $0.85 \\pm 0.07$ & $0.91 \\pm 0.05$ & $0.85 \\pm 0.07$ \\\\\n",
      "PatternAttribution & $0.99 \\pm 0.02$ & $0.81 \\pm 0.07$ & $0.98 \\pm 0.03$ & $0.80 \\pm 0.07$ & $0.95 \\pm 0.05$ & $0.82 \\pm 0.05$ & $0.95 \\pm 0.03$ & $0.83 \\pm 0.05$ & $0.99 \\pm 0.01$ & $0.93 \\pm 0.07$ & $0.98 \\pm 0.02$ & $0.95 \\pm 0.04$ \\\\\n",
      "SHAP & $0.91 \\pm 0.02$ & $0.85 \\pm 0.02$ & $0.88 \\pm 0.03$ & $0.85 \\pm 0.02$ & $0.88 \\pm 0.03$ & $0.82 \\pm 0.04$ & $0.88 \\pm 0.03$ & $0.82 \\pm 0.04$ & $0.87 \\pm 0.03$ & $0.79 \\pm 0.05$ & $0.86 \\pm 0.02$ & $0.82 \\pm 0.03$ \\\\\n",
      "Int. Grads. & $0.99 \\pm 0.00$ & $0.96 \\pm 0.01$ & $0.99 \\pm 0.00$ & $0.95 \\pm 0.01$ & $0.98 \\pm 0.01$ & $0.87 \\pm 0.00$ & $0.98 \\pm 0.01$ & $0.88 \\pm 0.00$ & $0.94 \\pm 0.03$ & $0.93 \\pm 0.03$ & $0.91 \\pm 0.03$ & $0.94 \\pm 0.03$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "}\n",
      "\\end{table}\n",
      "% --- End of LaTeX Table ---\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Transposed LaTeX tables\n",
    "# =========================\n",
    "def parse_scenario_name_for_table(scenario_base_name):\n",
    "    name_lower = scenario_base_name.lower()\n",
    "    base_type_str = \"OTHER\"\n",
    "    if \"xor\" in name_lower: base_type_str = \"XOR\"\n",
    "    elif \"multiplicative\" in name_lower: base_type_str = \"MULT\"\n",
    "    elif \"linear\" in name_lower or \"additive\" in name_lower: base_type_str = \"LIN\"\n",
    "    dist_str = \"DIST\" if (\"_distractor\" in name_lower or \"distractor_\" in name_lower) else \"\"\n",
    "    bg_str = \"UNK_BG\"\n",
    "    if \"white\" in name_lower: bg_str = \"WHITE\"\n",
    "    elif \"correlated\" in name_lower: bg_str = \"CORR\"\n",
    "    return f\"{base_type_str} {dist_str} {bg_str}\".replace(\"  \", \" \").strip()\n",
    "\n",
    "def _parse_label_parts(parsed_label):\n",
    "    parts = parsed_label.split()\n",
    "    base = parts[0]\n",
    "    if len(parts) == 2: return base, parts[1]\n",
    "    if len(parts) >= 3: return base, \" \".join(parts[1:])\n",
    "    return base, \"UNK_BG\"\n",
    "\n",
    "BASE_TYPES = [\"LIN\", \"MULT\", \"XOR\"]\n",
    "BG_ORDER = [\"WHITE\", \"CORR\", \"DIST WHITE\", \"DIST CORR\", \"UNK_BG\"]\n",
    "\n",
    "METHOD_DISPLAY = {\n",
    "    'pattern_gam': r'$\\text{PAT}^{\\text{GAM}}$',\n",
    "    'pattern_qlr': r'$\\text{PAT}^{\\text{QLR}}$',\n",
    "    'SD_NAM': r'$\\text{SD}(f^{\\text{GAM}})$',\n",
    "    'SDb_NAM': r'$\\text{SD}(f^{\\text{PGAM}})$',\n",
    "    'DISCR_NAM': r'$\\text{DISCR}(f^{\\text{GAM}})$',\n",
    "    'PROD_NAM': r'$\\text{PROD}(f^{\\text{GAM}})$',\n",
    "    'SD_QLR': r'$\\text{SD}(f^{\\text{QLR}})$',\n",
    "    'SDb_QLR': r'$\\text{SD}(f^{\\text{PQLR}})$',\n",
    "    'DISCR_QLR': r'$\\text{DISCR}(f^{\\text{QLR}})$',\n",
    "    'PROD_QLR': r'$\\text{PROD}(f^{\\text{QLR}})$',\n",
    "    'ebm': r'EBM',\n",
    "    'kernel_svm': r'Kernel Pattern',\n",
    "    'pattern_net': r'PatternNet',\n",
    "    'pattern_attribution': r'PatternAttribution',\n",
    "    'shap': r'SHAP',\n",
    "    'ig': r'Int. Grads.',\n",
    "}\n",
    "METHOD_ORDER = [\n",
    "    'pattern_gam', 'pattern_qlr',\n",
    "    'SD_NAM', 'SDb_NAM', 'PROD_NAM', 'DISCR_NAM',\n",
    "    'SD_QLR', 'SDb_QLR', 'PROD_QLR', 'DISCR_QLR',\n",
    "    'ebm', 'kernel_svm', 'pattern_net', 'pattern_attribution', 'shap', 'ig'\n",
    "]\n",
    "\n",
    "def format_latex_value(mean_val, std_val, precision=2, is_bold=False):\n",
    "    if mean_val is None or std_val is None or np.isnan(mean_val) or np.isnan(std_val):\n",
    "        return \"-\"\n",
    "    mean_str = f\"{mean_val:.{precision}f}\"\n",
    "    std_str  = f\"{std_val:.{precision}f}\"\n",
    "    core = f\"{mean_str} \\\\pm {std_str}\"\n",
    "    return f\"$\\\\mathbf{{{core}}}$\" if is_bold else f\"${core}$\"\n",
    "\n",
    "def generate_latex_tables_transposed(aggregated_metrics, precision=2, shade_mult=True):\n",
    "    data_for_tables = {}\n",
    "    present_methods, present_labels = set(), set()\n",
    "    for scenario_base_name, methods_data in aggregated_metrics.items():\n",
    "        parsed_label = parse_scenario_name_for_table(scenario_base_name)\n",
    "        present_labels.add(parsed_label)\n",
    "        data_for_tables.setdefault(parsed_label, {})\n",
    "        for method_name, metrics_values in methods_data.items():\n",
    "            present_methods.add(method_name)\n",
    "            data_for_tables[parsed_label][method_name] = metrics_values\n",
    "\n",
    "    columns = []\n",
    "    group_spans = []\n",
    "    for bt in BASE_TYPES:\n",
    "        subcols = []\n",
    "        for bg in BG_ORDER:\n",
    "            label = f\"{bt} {bg}\".replace(\"  \",\" \").strip()\n",
    "            if label in present_labels:\n",
    "                subcols.append(label)\n",
    "        if subcols:\n",
    "            columns.extend(subcols)\n",
    "            group_spans.append((bt, len(subcols)))\n",
    "\n",
    "    ordered_methods = [m for m in METHOD_ORDER if m in present_methods]\n",
    "    extras = sorted([m for m in present_methods if m not in ordered_methods])\n",
    "    ordered_methods.extend(extras)\n",
    "\n",
    "    metrics_config = [\n",
    "        (\"IMA\", \"IMA_mean\", \"IMA_std\", \"Importance Mass Accuracy (IMA)\"),\n",
    "        (\"EMD\", \"EMD_mean\", \"EMD_std\", \"Earth Mover's Distance (EMD)\"),\n",
    "        (\"FNI-EMD\", \"FNI_EMD_mean\", \"FNI_EMD_std\", \"False-Negative Invariant EMD (FNI-EMD)\")\n",
    "    ]\n",
    "\n",
    "    latex_output_tables = {}\n",
    "\n",
    "    for metric_key, mean_key, std_key, caption_title in metrics_config:\n",
    "        colspec = \"l|\"\n",
    "        for col_label in columns:\n",
    "            base_type, _ = _parse_label_parts(col_label)\n",
    "            colspec += \"M\" if (shade_mult and base_type == \"MULT\") else \"c\"\n",
    "        table_cols_format = colspec\n",
    "\n",
    "        top_header_cells = [\"\"]\n",
    "        for group_name, span in group_spans:\n",
    "            if shade_mult and group_name == \"MULT\":\n",
    "                top_header_cells.append(rf\"\\multicolumn{{{span}}}{{c}}{{\\cellcolor{{colMULT}} {group_name}}}\")\n",
    "            else:\n",
    "                top_header_cells.append(rf\"\\multicolumn{{{span}}}{{c}}{{{group_name}}}\")\n",
    "        top_header_line = \" & \".join(top_header_cells) + r\" \\\\\"\n",
    "\n",
    "        sub_header_cells = [\"Method\"]\n",
    "        for col_label in columns:\n",
    "            _, bg_group = _parse_label_parts(col_label)\n",
    "            sub_header_cells.append(bg_group)\n",
    "        sub_header_line = \" & \".join(sub_header_cells) + r\" \\\\ \\hline\\hline\"\n",
    "\n",
    "        col_best_rounded = []\n",
    "        for col_label in columns:\n",
    "            vals = []\n",
    "            for m in ordered_methods:\n",
    "                mean_val = data_for_tables.get(col_label, {}).get(m, {}).get(mean_key)\n",
    "                if mean_val is not None and not np.isnan(mean_val):\n",
    "                    vals.append(np.around(mean_val, precision))\n",
    "            col_best_rounded.append(np.max(vals) if vals else None)\n",
    "\n",
    "        s = []\n",
    "        s.append(f\"% Transposed LaTeX Table for {metric_key} (MULT banded)\")\n",
    "        s.append(r\"\\begin{table}[htbp]\")\n",
    "        s.append(r\"\\centering\")\n",
    "        s.append(rf\"\\caption{{{caption_title}. Values are mean $\\pm$ standard deviation. Best result per column (ties at {precision} dp) is emboldened.}}\")\n",
    "        s.append(rf\"\\label{{tab:{metric_key.lower().replace('-', '')}_results_transposed}}\")\n",
    "        s.append(r\"\\resizebox{\\textwidth}{!}{\")\n",
    "        s.append(rf\"\\begin{{tabular}}{{{table_cols_format}}}\")\n",
    "        s.append(r\"\\hline\")\n",
    "        s.append(top_header_line)\n",
    "        s.append(sub_header_line)\n",
    "\n",
    "        for m in ordered_methods:\n",
    "            disp = METHOD_DISPLAY.get(m, m.replace(\"_\", r\"\\_\"))\n",
    "            row_cells = [disp]\n",
    "            for j, col_label in enumerate(columns):\n",
    "                md = data_for_tables.get(col_label, {}).get(m, {})\n",
    "                mean_val = md.get(mean_key); std_val = md.get(std_key)\n",
    "                is_best = (\n",
    "                    mean_val is not None and not np.isnan(mean_val) and\n",
    "                    col_best_rounded[j] is not None and\n",
    "                    np.around(mean_val, precision) == col_best_rounded[j]\n",
    "                )\n",
    "                row_cells.append(format_latex_value(mean_val, std_val, precision=precision, is_bold=is_best))\n",
    "            s.append(\" & \".join(row_cells) + r\" \\\\\")\n",
    "        s.append(r\"\\hline\")\n",
    "        s.append(r\"\\end{tabular}\")\n",
    "        s.append(r\"}\")\n",
    "        s.append(r\"\\end{table}\")\n",
    "\n",
    "        latex_output_tables[metric_key] = \"\\n\".join(s)\n",
    "\n",
    "    return latex_output_tables\n",
    "\n",
    "latex_tables = generate_latex_tables_transposed(final_aggregated_results, precision=2, shade_mult=True)\n",
    "for metric_name, table_code in latex_tables.items():\n",
    "    print(f\"\\n% --- LaTeX Table for {metric_name} (transposed, MULT banded) ---\")\n",
    "    print(table_code)\n",
    "    print(\"% --- End of LaTeX Table ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
